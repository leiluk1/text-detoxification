{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline solution\n",
    "\n",
    "## Delete\n",
    "The concept for this approach draws inspiration from how mass media handles offensive language. For instance, in TV programs and articles, swear words are often obscured or substituted with asterisks (`*`). Following a similar principle, goal of this baseline is to remove offensive words from the source text. \n",
    "\n",
    "To perform the baseline solution, I have created a dictionary containing the most frequently used swear words from the obscenity list taken from [here](https://github.com/surge-ai/profanity/tree/main). Using nltk library, I cleaned the reference sentences. Any words that match the words in that dictionary of swear words are then removed filtering out toxic language.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/leiluk1/innopolis/text-detoxification\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/leiluk1/innopolis/text-detoxification/.venv/lib/python3.9/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2.0 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "[nltk_data] Downloading package punkt to /Users/leiluk1/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%cd ..\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import requests\n",
    "import io\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dictionary of profanity words\n",
    "def download_csv_and_save_column(url, file_path):\n",
    "    # Download the CSV file using the link\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Load the CSV data into a DataFrame\n",
    "    df = pd.read_csv(io.StringIO(response.text))\n",
    "    \n",
    "    # Save the dictionary to a text file (+ add some toxic words that not present in dictionary)\n",
    "    profanity_vocab = set(df['canonical_form_1'].to_list() + ['idiot', 'stupid', 'fucking', 'nigga'])\n",
    "    with open(file_path, 'w') as file:\n",
    "        for word in profanity_vocab:\n",
    "            file.write(word + '\\n')\n",
    "    \n",
    "\n",
    "vocab_url = 'https://github.com/surge-ai/profanity/raw/main/profanity_en.csv'\n",
    "vocab_path = './data/external/profanity_vocab.txt'\n",
    "download_csv_and_save_column(vocab_url, vocab_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['grope', 'creampie', 'jizz', 'snatch', 'queer', 'kike']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the dictionary of toxic words \n",
    "with open(vocab_path) as vocab_file:\n",
    "        dictionary = [line.strip().split()[0] for line in vocab_file.readlines()]\n",
    "\n",
    "dictionary[:6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since for this baseline training process is not required, I will perform it only on test set and then save the results for further evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/interim/test.csv')\n",
    "\n",
    "source = df['reference'].to_list()\n",
    "target = df['detox_reference'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_toxic_words(texts, bad_words):\n",
    "    \"\"\"\n",
    "    Deletes all toxic words from a pre-defined vocab in a list of source texts,\n",
    "    and writes the resulting texts to an array.\n",
    "\n",
    "    Args:\n",
    "        texts: A list of strings containing the source texts to be cleaned.\n",
    "        bad_words: The obscene words dictionary.\n",
    "\n",
    "    Returns:\n",
    "        A list of cleaned texts with toxic words removed.\n",
    "    \"\"\"\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for text in tqdm(texts, desc=\"Deleting toxic words...\"):\n",
    "        # Tokenize the text into words\n",
    "        text = word_tokenize(text)\n",
    "        \n",
    "        # Remove all toxic words from the text\n",
    "        clean_text = ' '.join(\n",
    "            [word for word in text if word.lower() not in bad_words]\n",
    "        )\n",
    "        \n",
    "        # Remove extra spaces before punctuation\n",
    "        clean_text = re.sub(r'\\s+([^\\w\\s])', r'\\1', clean_text)\n",
    "        \n",
    "        results.append(clean_text)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deleting toxic words...: 100%|██████████| 5000/5000 [00:00<00:00, 12610.09it/s]\n"
     ]
    }
   ],
   "source": [
    "result = delete_toxic_words(source, bad_words=dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the results to a CSV file\n",
    "df['baseline_result'] = result\n",
    "\n",
    "resulting_dataset_path = './data/interim/baseline_results.csv'\n",
    "df.to_csv(resulting_dataset_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deleting toxic words...: 100%|██████████| 2/2 [00:00<00:00, 11966.63it/s]\n"
     ]
    }
   ],
   "source": [
    "examples = [\"What a stupid joke.\", \"Fucking damn joke!\"]\n",
    "baseline_result = delete_toxic_words(examples, bad_words=dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: What a stupid joke.\n",
      "Baseline result: What a joke.\n",
      "\n",
      "Source: Fucking damn joke!\n",
      "Baseline result: joke!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, example in enumerate(examples):\n",
    "    print('Source:', example)\n",
    "    print('Baseline result:', baseline_result[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen, the bad words from the source sentences were removed. However, this approach is not the perfect one as it may result in the loss of contextual information within the original toxic sentence."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
