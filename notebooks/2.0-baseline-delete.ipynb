{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline solution\n",
    "\n",
    "## Delete\n",
    "The concept for this approach draws inspiration from how mass media handles offensive language. For instance, in TV programs and articles, swear words are often obscured or substituted with asterisks (`**`). Following a similar principle, goal of this baseline is to remove offensive words from the source text. \n",
    "\n",
    "To perform the baseline solution, I have created a dictionary containing the most frequently used swear words from the obscenity list taken from [here](https://github.com/surge-ai/profanity/tree/main). Using nltk library, I cleaned the reference sentences. Any words that match the words in that dictionary of swear words are then removed filtering out toxic language.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ..\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import requests\n",
    "import io\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dictionary of profanity words\n",
    "def download_csv_and_save_column(url, file_path):\n",
    "    # Download the CSV file using the link\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Load the CSV data into a DataFrame\n",
    "    df = pd.read_csv(io.StringIO(response.text))\n",
    "    \n",
    "    # Save the dictionary to a text file (+ add some toxic words that not present in dictionary)\n",
    "    profanity_vocab = set(df['canonical_form_1'].to_list() + ['idiot', 'stupid', 'fucking', 'nigga'])\n",
    "    with open(file_path, 'w') as file:\n",
    "        for word in profanity_vocab:\n",
    "            file.write(word + '\\n')\n",
    "    \n",
    "\n",
    "vocab_url = 'https://github.com/surge-ai/profanity/raw/main/profanity_en.csv'\n",
    "vocab_path = './data/external/profanity_vocab.txt'\n",
    "download_csv_and_save_column(vocab_url, vocab_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gypsy', 'upskirt', 'tadger', 'tossing', 'shit', 'bean']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the vocab of toxic words \n",
    "with open(vocab_path) as vocab_file:\n",
    "        vocab = [line.strip().split()[0] for line in vocab_file.readlines()]\n",
    "\n",
    "vocab[:6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since for this baseline training process is not required, I will perform it only on test set and then save the results for further evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/interim/test.csv')\n",
    "\n",
    "source = df['reference'].to_list()\n",
    "target = df['detox_reference'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_toxic_words(texts, vocab):\n",
    "    \"\"\"\n",
    "    Deletes all toxic words from a pre-defined vocab in a list of source texts,\n",
    "    and writes the resulting texts to an array.\n",
    "\n",
    "    Args:\n",
    "        texts (list): A list of strings containing the source texts to be cleaned.\n",
    "        vocab (list): The list of obscene words.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of cleaned texts with toxic words removed.\n",
    "    \"\"\"\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for text in tqdm(texts, desc=\"Deleting toxic words\"):\n",
    "        # Tokenize the text into words\n",
    "        text = word_tokenize(text)\n",
    "        \n",
    "        # Remove all toxic words from the text\n",
    "        clean_text = ' '.join(\n",
    "            [word for word in text if word.lower() not in vocab]\n",
    "        )\n",
    "        \n",
    "        # Remove extra spaces before punctuation\n",
    "        clean_text = re.sub(r'\\s+([^\\w\\s])', r'\\1', clean_text)\n",
    "        \n",
    "        results.append(clean_text)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deleting toxic words: 100%|██████████| 20000/20000 [00:01<00:00, 14065.83it/s]\n"
     ]
    }
   ],
   "source": [
    "result = delete_toxic_words(source, vocab=vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the results to a CSV file\n",
    "df['baseline_result'] = result\n",
    "\n",
    "resulting_dataset_path = './data/interim/baseline_results.csv'\n",
    "df.to_csv(resulting_dataset_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: I was there when he took a photo to your super penis.\n",
      "Baseline res: I was there when he took a photo to your super.\n",
      "Target: I was there when he took a picture next to your superbird. \n",
      "\n",
      "Source: This shit's temporary.\n",
      "Baseline res: This's temporary.\n",
      "Target: it's temporary. \n",
      "\n",
      "Source: why, you little bastard!\n",
      "Baseline res: why, you little!\n",
      "Target: Why you little! \n",
      "\n",
      "Source: when this vampire shit moves, people either start dating again, or we all get killed.\n",
      "Baseline res: when this vampire moves, people either start dating again, or we all get killed.\n",
      "Target: 'Cause once this vampire situation is over, folk gonna be back or we gonna be dead. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in [5, 15, 25, 50]:\n",
    "    print('Source:', source[i])\n",
    "    print('Baseline res:', result[i])\n",
    "    print('Target:', target[i], '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, this approach is not the perfect one as it may result in the loss of contextual information within the original toxic sentence."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
